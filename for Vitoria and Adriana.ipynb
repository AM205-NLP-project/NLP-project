{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ef16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 11\n",
      "Logits (aka language modeling predictions) shape: torch.Size([1, 11, 50257])\n",
      "Hidden states (aka latent/internal representation) tuple length: 13\n",
      "Initial embedding representation (before contextualization) shape: torch.Size([1, 11, 768])\n",
      "Final representation from GPT-2 shape: torch.Size([1, 11, 768])\n",
      "\tLayer 0 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 1 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 2 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 3 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 4 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 5 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 6 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 7 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 8 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 9 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 10 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 11 representation shape: torch.Size([1, 11, 768])\n",
      "\tLayer 12 representation shape: torch.Size([1, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "def sample_from_model(text):\n",
    "    # how to sample from the model\n",
    "    # I recommend checking out this guide: https://huggingface.co/blog/how-to-generate\n",
    "    # sampling is very, very tricky and is still very much unsolved\n",
    "    # see this paper for discussion: https://arxiv.org/pdf/1904.09751.pdf\n",
    "    generated = torch.tensor(tok.encode(text)).unsqueeze(0)\n",
    "    # generated = generated.to(device)\n",
    "\n",
    "    # sample model\n",
    "    sample_outputs = model.generate(\n",
    "        # input seed; any pre-generation text we want to start with\n",
    "        generated,\n",
    "\n",
    "        # just setting the pad token to the EOS token for sampling\n",
    "        pad_token_id=50256,\n",
    "\n",
    "        # sample text using probabilities as opposed to greedy sampling / beam search\n",
    "        do_sample=True,\n",
    "\n",
    "        # higher temperature = sampling rarer words/tokens; low temperature = more conservative sample\n",
    "        # temperature=0.9,\n",
    "\n",
    "        # cap on the number of tokens that will be generated\n",
    "        # currently have this set so that the model can generate up to 10 more tokens\n",
    "        max_length=generated.shape[1] + 10,\n",
    "\n",
    "        # forcing the model to generate at least 2 more tokens\n",
    "        min_length=generated.shape[1] + 2,\n",
    "\n",
    "        # sampling modalities\n",
    "        # top_k only retains the top k tokens and randomly samples from thsoe\n",
    "        # top_k=200,\n",
    "\n",
    "        # top p = nucleus sampling\n",
    "        # defined in terms of probabilities...\n",
    "        # only retains the top words until the specified probability region is reached\n",
    "        top_p=0.95,\n",
    "\n",
    "        # how many samples to generate\n",
    "        num_return_sequences=10,\n",
    "\n",
    "        # params for beam search\n",
    "        # number of hypotheses to evaluate in parallel\n",
    "        # num_beams=5,\n",
    "\n",
    "        # penalty for repeating ngrams in beam search\n",
    "        # no_repeat_ngram_size=2,\n",
    "\n",
    "        # quits beam search early if all beams have hit an EOS token\n",
    "        # early_stopping=True,\n",
    "    )\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        ox = tok.decode(sample_output, skip_special_tokens=True)\n",
    "        out = \"{}: {}\".format(i, ox)\n",
    "        print(out)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = GPT2LMHeadModel.from_pretrained('model_unique_best/')\n",
    "    tok = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tok.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "    \n",
    "    # **********************\n",
    "    # REPLACE SENTENCE BELOW\n",
    "    test_txt = 'to operate a vehicle, controlling its motion Word: '\n",
    "    # **********************\n",
    "    \n",
    "    tokens = tok.tokenize(test_txt)\n",
    "    model_inp = tok(test_txt, return_tensors='pt')\n",
    "    print('Num tokens:', len(tokens))\n",
    "\n",
    "    outputs = model(**model_inp, output_hidden_states=True)\n",
    "    logits = outputs.logits\n",
    "    print('Logits (aka language modeling predictions) shape:', logits.shape)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    print('Hidden states (aka latent/internal representation) tuple length:', len(hidden_states))\n",
    "\n",
    "    initial_embed = hidden_states[0]\n",
    "    print('Initial embedding representation (before contextualization) shape:', initial_embed.shape)\n",
    "\n",
    "    final_rep = hidden_states[-1]\n",
    "    print('Final representation from GPT-2 shape:', final_rep.shape)\n",
    "\n",
    "    for ix, rep in enumerate(hidden_states):\n",
    "        print('\\tLayer', ix, 'representation shape:', rep.shape)\n",
    "\n",
    "    #sample_from_model(test_txt)\n",
    "    #sample_from_model('Example: I am running so fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a87791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0266, -0.4240,  0.1520,  ...,  0.0091,  0.1409,  0.0453],\n",
       "         [ 0.2042, -0.0241,  0.0055,  ...,  0.0604, -0.1768,  0.1519],\n",
       "         [-0.0492, -0.0615,  0.0961,  ...,  0.0639,  0.0917, -0.0516],\n",
       "         ...,\n",
       "         [-0.2003, -0.0107,  0.3483,  ...,  0.0400, -0.0633, -0.2089],\n",
       "         [ 0.0466, -0.1890,  0.1468,  ...,  0.0198, -0.0906, -0.0346],\n",
       "         [ 0.1001, -0.0854,  0.1697,  ..., -0.0434, -0.1635,  0.1311]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll likely want to use information from layer 0 or layer 12. \n",
    "\n",
    "# Get first layer's encoding: \n",
    "hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cde6e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2599, -1.1141, -0.3229,  ...,  0.0250, -0.1107,  0.0189],\n",
       "         [-0.3212, -0.4170, -1.9204,  ...,  0.3450,  0.0714,  0.1612],\n",
       "         [-0.1505,  0.2780,  0.7379,  ...,  0.0901, -0.2187, -0.2929],\n",
       "         ...,\n",
       "         [-1.1537,  0.6237, -1.5018,  ..., -0.3143, -0.2512,  0.3086],\n",
       "         [ 0.0774, -0.4346, -0.2052,  ...,  0.1735, -0.0155,  0.7894],\n",
       "         [-0.3195, -0.2111, -0.0912,  ...,  0.8415,  0.0648,  0.1905]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get last layer's encoding: \n",
    "hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6f095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
